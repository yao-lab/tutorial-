{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib  inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import torch\n",
    "import time\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "#import basic_cnn\n",
    "from collections import OrderedDict #if import basic_cnn, this line can be dropped\n",
    "from utils import AverageMeter, accuracy, get_margin\n",
    "\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "use_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "geom_tight_frame = torch.empty(18,3,3,dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "geom_tight_frame[0,:,:]=torch.tensor([[1,2,1],[2,4,2],[1,2,1]],dtype=torch.float)/16\n",
    "geom_tight_frame[1,:,:]=torch.tensor([[1,0,-1],[2,0,-2],[1,0,-1]],dtype=torch.float)/16\n",
    "geom_tight_frame[2,:,:]=torch.tensor([[1,2,1],[0,0,0],[-1,-2,-1]],dtype=torch.float)/16\n",
    "geom_tight_frame[3,:,:]=torch.tensor([[1,1,0],[1,0,-1],[0,-1,-1]],dtype=torch.float)*np.sqrt(2)/16\n",
    "geom_tight_frame[4,:,:]=torch.tensor([[0,1,1],[-1,0,1],[-1,-1,0]],dtype=torch.float)*np.sqrt(2)/16\n",
    "geom_tight_frame[5,:,:]=torch.tensor([[1,0,-1],[0,0,0],[-1,0,1]],dtype=torch.float)*np.sqrt(7)/24\n",
    "geom_tight_frame[6,:,:]=torch.tensor([[-1,2,-1],[-2,4,-2],[-1,2,-1]],dtype=torch.float)/48\n",
    "geom_tight_frame[7,:,:]=torch.tensor([[-1,-2,-1],[2,4,2],[-1,-2,-1]],dtype=torch.float)/48\n",
    "geom_tight_frame[8,:,:]=torch.tensor([[0,0,-1],[0,2,0],[-1,0,0]],dtype=torch.float)/12\n",
    "geom_tight_frame[9,:,:]=torch.tensor([[-1,0,0],[0,2,0],[0,0,-1]],dtype=torch.float)/12\n",
    "geom_tight_frame[10,:,:]=torch.tensor([[0,1,0],[-1,0,-1],[0,1,0]],dtype=torch.float)*np.sqrt(2)/12\n",
    "geom_tight_frame[11,:,:]=torch.tensor([[-1,0,1],[2,0,-2],[-1,0,1]],dtype=torch.float)*np.sqrt(2)/16\n",
    "geom_tight_frame[12,:,:]=torch.tensor([[-1,2,-1],[0,0,0],[1,-2,1]],dtype=torch.float)*np.sqrt(2)/16\n",
    "geom_tight_frame[13,:,:]=torch.tensor([[1,-2,1],[-2,4,-2],[1,-2,1]],dtype=torch.float)/48\n",
    "geom_tight_frame[14,:,:]=torch.tensor([[0,0,0],[-1,2,-1],[0,0,0]],dtype=torch.float)*np.sqrt(2)/12\n",
    "geom_tight_frame[15,:,:]=torch.tensor([[-1,2,-1],[0,0,0],[-1,2,-1]],dtype=torch.float)*np.sqrt(2)/24\n",
    "geom_tight_frame[16,:,:]=torch.tensor([[0,-1,0],[0,2,0],[0,-1,0]],dtype=torch.float)*np.sqrt(2)/12\n",
    "geom_tight_frame[17,:,:]=torch.tensor([[-1,0,-1],[2,0,2],[-1,0,-1]],dtype=torch.float)*np.sqrt(2)/24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tight frame tensor:  torch.Size([18, 1, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "geom_tight_frame = geom_tight_frame.view([18,1,1,3,3])\n",
    "print('tight frame tensor: ',geom_tight_frame.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0625,  0.1250,  0.0625],\n",
      "        [ 0.1250,  0.2500,  0.1250],\n",
      "        [ 0.0625,  0.1250,  0.0625]])\n"
     ]
    }
   ],
   "source": [
    "print(geom_tight_frame[0,0,0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How to construct a 5-layer CNN where each convolutional layer is a linear combination of geometric tight frames above?\n",
    "class CNN_frame(nn.Module):\n",
    "    def __init__(self, channels, filters, output_size, with_bn=True):\n",
    "        super(CNN_frame, self).__init__()\n",
    "        self.with_bn = with_bn\n",
    "        self.features = self._make_layers(channels)\n",
    "        self.classifier = nn.Linear(channels, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, channels):\n",
    "        layers = []\n",
    "        in_channels = 1 #in_channels = 3\n",
    "        for i in range(1): #for i in range(5):\n",
    "            if i == 0:\n",
    "                if self.with_bn:\n",
    "                    layers += [('conv%dB' % i, nn.Conv3d(in_channels, filters, [1,3,3], stride=2, padding=[0,1,1])),\n",
    "                               ('conv%dW' % i, nn.Conv3d(filters, channels, [3,1,1], stride=1, padding=0)),\n",
    "                               ('bn%d' % i, nn.BatchNorm2d(channels)),\n",
    "                               ('relu%d' % i, nn.ReLU(inplace=True))]\n",
    "                else:\n",
    "                    layers += [('conv%dB' % i, nn.Conv3d(in_channels, filters, [1,3,3], stride=2, padding=[0,1,1])),\n",
    "                               ('conv%dW' % i, nn.Conv3d(filters, channels, [3,1,1], stride=1, padding=0)),#('conv%d' % i, nn.Conv2d(in_channels, channels, 3, 2, 1)),\n",
    "                               ('relu%d' % i, nn.ReLU(inplace=True))]\n",
    "            else:\n",
    "                if self.with_bn:\n",
    "                    layers += [('conv%dB' % i, nn.Conv3d(in_channels, filters, [1,3,3], stride=2, padding=[0,1,1])),\n",
    "                               ('conv%dW' % i, nn.Conv3d(filters, channels, [3,1,1], stride=1, padding=0)), #('conv%d' % i, nn.Conv2d(channels, channels, 3, 2, 1)),\n",
    "                               ('bn%d' % i, nn.BatchNorm2d(channels)),\n",
    "                               ('relu%d' % i, nn.ReLU(inplace=True))]\n",
    "                else:\n",
    "                    layers += [('conv%dB' % i, nn.Conv3d(in_channels, filters, [1,3,3], stride=2, padding=[0,1,1])),\n",
    "                               ('conv%dW' % i, nn.Conv3d(filters, channels, [3,1,1], stride=1, padding=0)),#('conv%d' % i, nn.Conv2d(channels, channels, 3, 2, 1)),\n",
    "                               ('relu%d' % i, nn.ReLU(inplace=True))]\n",
    "        return nn.Sequential(OrderedDict(layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training with Cross-Entropy Loss\n",
    "def train_model(model, criterion, optimizer, log_saver, num_epochs=100, margin_dist_ind=[]):\n",
    "    since = time.time()\n",
    "    steps = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        margin = []\n",
    "\n",
    "        for phase in ['train', 'test']:\n",
    "\n",
    "            loss_meter = AverageMeter()\n",
    "            acc_meter = AverageMeter()\n",
    "\n",
    "            if phase == 'train':\n",
    "                model.train(True)\n",
    "            else:\n",
    "                model.train(False)\n",
    "\n",
    "            for i, data in enumerate(loaders[phase]):\n",
    "                inputs, labels = data\n",
    "                if use_gpu:\n",
    "                    inputs = inputs.cuda()\n",
    "                    labels = labels.cuda()\n",
    "                inputs = Variable(inputs)\n",
    "                labels = Variable(labels)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # The inputs are of size [batch_size,in_channels,H,W], and is changed to [batch_size,1,in_channels,H,W]\n",
    "                inputs = inputs.unsqueeze(1)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    steps += 1\n",
    "                    margin = np.append(margin, get_margin(outputs, labels))\n",
    "\n",
    "                loss_meter.update(loss.data.item(), outputs.size(0))\n",
    "                acc_meter.update(accuracy(outputs.data, labels.data)[-1].item(), outputs.size(0))\n",
    "\n",
    "            epoch_loss = loss_meter.avg\n",
    "            epoch_error = 1 - acc_meter.avg / 100\n",
    "\n",
    "            if phase == 'train':\n",
    "\n",
    "                log_saver['train_loss'].append(epoch_loss)\n",
    "                log_saver['train_error'].append(epoch_error)\n",
    "                log_saver['margin'].append(min(margin))\n",
    "                #log_saver['margin'].append(margin)\n",
    "                ww = 1\n",
    "                for i in range(6):\n",
    "                    if i <= 4:\n",
    "                        size = eval('model.features.conv%d.weight.size()' % i)\n",
    "                        # here to compute the F norm between each CNN layer\n",
    "                        #w_norm = eval('model.features.conv%d.weight.view(size[0],-1).pow(2).sum(1).mean().data.item()' % i)\n",
    "                        #model.features.conv0.weight*model.features.bn0.weight[:,None,None,None]/(model.features.bn0.running_var.sqrt()[:,None,None,None]+1e-5)\n",
    "                        scaled_w = eval('model.features.conv%d.weight*model.features.bn%d.weight[:,None,None,None]/(model.features.bn%d.running_var.sqrt()[:,None,None,None]+1e-5)' % (i,i,i))\n",
    "                        w_norm = scaled_w.view(size[0],-1).pow(2).sum().sqrt().data.item()\n",
    "\n",
    "                        log_saver['w%d' % i].append(w_norm)\n",
    "                    else:\n",
    "                        w_norm = model.classifier.weight.norm(2).data.item()\n",
    "                        log_saver['w_fc'].append(w_norm)\n",
    "                    ww *= w_norm\n",
    "                log_saver['normalised_margin'].append(log_saver['margin'][-1] / ww)\n",
    "                if epoch in margin_dist_ind: \n",
    "                    log_saver['normalized_margin_dist'].append(margin/ww)\n",
    "                    print('Normalized Margin Distribution saved.')\n",
    "\n",
    "            elif phase == 'test':\n",
    "\n",
    "                log_saver['test_loss'].append(epoch_loss)\n",
    "                log_saver['test_error'].append(epoch_error)\n",
    "\n",
    "            print('{} Loss: {:.4f} Error: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_error), end=' ' if phase == 'train' else '\\n')\n",
    "            if phase == 'train':\n",
    "                print('w4_norm: {:.4f} Margin: {:.4f} Norm_margin: {:.4f}'.format(log_saver['w4'][-1],\n",
    "                                                                                  log_saver['margin'][-1],\n",
    "                                                                                  log_saver['normalised_margin'][-1]))\n",
    "\n",
    "        if epoch % 30 == 0 or epoch == num_epochs - 1:\n",
    "            print('Saving..')\n",
    "            state = {\n",
    "                'net': model,\n",
    "                'epoch': epoch,\n",
    "                'log': log_saver\n",
    "            }\n",
    "\n",
    "            if not os.path.isdir('checkpoint_CNN'):\n",
    "                os.mkdir('checkpoint_CNN')\n",
    "            torch.save(state, './checkpoint_CNN/ckpt_epoch_{}.t7'.format(epoch))\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    return model, log_saver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = './'\n",
    "lr = 0.01\n",
    "BATCH_SIZE = 100\n",
    "weight_decay = 0.\n",
    "\n",
    "img_transforms = transforms.Compose([transforms.ToTensor(),\n",
    "                                     transforms.Normalize(\n",
    "                                         (0.4914, 0.4822, 0.4465),\n",
    "                                         (0.2023, 0.1994, 0.2010))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "training data size: 50000\n",
      "testing data size: 10000\n"
     ]
    }
   ],
   "source": [
    "training_dataset = datasets.CIFAR10(root, train=True, transform=img_transforms, download=True)\n",
    "training_loader = DataLoader(training_dataset, BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "\n",
    "testing_dataset = datasets.CIFAR10(root, train=False, transform=img_transforms)\n",
    "testing_loader = DataLoader(testing_dataset, BATCH_SIZE, shuffle=False, pin_memory=True)\n",
    "\n",
    "loaders = {'train': training_loader, 'test': testing_loader}\n",
    "\n",
    "print('training data size:',len(training_dataset))\n",
    "print('testing data size:',len(testing_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size: torch.Size([100, 3, 32, 32])\n",
      "input size changed: torch.Size([100, 1, 3, 32, 32])\n",
      "tight frame tensor:  torch.Size([18, 1, 1, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "phase='test'\n",
    "for i, data in enumerate(loaders[phase]):\n",
    "    inputs, labels = data\n",
    "    if use_gpu:\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "    inputs = Variable(inputs)\n",
    "    labels = Variable(labels)\n",
    "\n",
    "print('input size:',inputs.size())\n",
    "inputs1 = inputs.unsqueeze(1)\n",
    "print('input size changed:',inputs1.size())\n",
    "geom_tight_frame = geom_tight_frame.unsqueeze(1)\n",
    "print('tight frame tensor: ',geom_tight_frame.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number-of-parameters: [1530]\n",
      "All parameter_name: features.conv0B.weight\n",
      "All parameter_name: features.conv0B.bias\n",
      "All parameter_name: features.conv0W.weight\n",
      "All parameter_name: features.conv0W.bias\n",
      "All parameter_name: features.bn0.weight\n",
      "All parameter_name: features.bn0.bias\n",
      "All parameter_name: features.bn0.running_mean\n",
      "All parameter_name: features.bn0.running_var\n",
      "All parameter_name: classifier.weight\n",
      "All parameter_name: classifier.bias\n",
      "Gradient updated parameter_name: features.conv0B.weight\n",
      "Gradient updated parameter_name: features.conv0B.bias\n",
      "Gradient updated parameter_name: features.conv0W.weight\n",
      "Gradient updated parameter_name: features.conv0W.bias\n",
      "Gradient updated parameter_name: features.bn0.weight\n",
      "Gradient updated parameter_name: features.bn0.bias\n",
      "Gradient updated parameter_name: classifier.weight\n",
      "Gradient updated parameter_name: classifier.bias\n"
     ]
    }
   ],
   "source": [
    "log = {'num_params': [],\n",
    "       'train_loss': [],\n",
    "       'train_error': [],\n",
    "       'test_loss': [],\n",
    "       'test_error': [],\n",
    "       'w0': [], 'w1': [], 'w2': [],\n",
    "       'w3': [], 'w4': [], 'w_fc': [],\n",
    "       'margin': [], 'normalised_margin': []}\n",
    "\n",
    "# %% run the model\n",
    "num_epochs = 100\n",
    "channels = 20\n",
    "filters = 18\n",
    "outputs = 10\n",
    "# here use with_bn to control batch normalisation\n",
    "model = CNN_frame(channels, filters, outputs, with_bn=True)\n",
    "\n",
    "number_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "log['num_params'].append(number_params)\n",
    "\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "\n",
    "print('number-of-parameters:',log['num_params'])\n",
    "        \n",
    "for name1, param1 in model.state_dict().items():\n",
    "    print('All parameter_name:',name1)\n",
    "    \n",
    "\n",
    "for name, param in model.named_parameters(): #for name, param in model.state_dict().items():\n",
    "    if param.requires_grad:\n",
    "        print('Gradient updated parameter_name:',name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can not use eval for = expression(赋值), replace it by exec\n",
    "exec('model.features.conv%dB.weight.data = geom_tight_frame' % 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Basis size: torch.Size([18, 1, 1, 1, 3, 3])\n",
      "\n",
      " Weight size: torch.Size([20, 18, 3, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print('\\n Basis size:', model.features.conv0B.weight.size())\n",
    "print('\\n Weight size:', model.features.conv0W.weight.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000, -0.1179,  0.0000],\n",
      "         [ 0.0000,  0.2357,  0.0000],\n",
      "         [ 0.0000, -0.1179,  0.0000]]])\n",
      "Update param:  tensor([[-0.0356, -0.0425,  0.0313],\n",
      "        [-0.0125, -0.1255,  0.0145],\n",
      "        [-0.0791,  0.0447,  0.0653],\n",
      "        [ 0.0146,  0.0280, -0.0228],\n",
      "        [ 0.1057, -0.0852, -0.1158],\n",
      "        [-0.0092,  0.0310,  0.0296],\n",
      "        [-0.1224,  0.1120, -0.1314],\n",
      "        [ 0.0098,  0.0595, -0.1202],\n",
      "        [ 0.1165,  0.0633, -0.0911],\n",
      "        [-0.0489,  0.1095,  0.1307],\n",
      "        [-0.0659, -0.0733, -0.0567],\n",
      "        [ 0.0225, -0.0011, -0.0801],\n",
      "        [ 0.1216, -0.0115,  0.0423],\n",
      "        [-0.0291, -0.0435, -0.1230],\n",
      "        [ 0.1124, -0.1298,  0.1149],\n",
      "        [ 0.1270,  0.1320, -0.0089],\n",
      "        [-0.1343, -0.0950,  0.0845],\n",
      "        [ 0.1348,  0.0974,  0.0648]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.features.conv0B.weight.data = geom_tight_frame\n",
    "model.features.conv0B.weight.requires_grad = False\n",
    "model.features.conv0B.bias.data = torch.zeros([18],dtype=torch.float)\n",
    "model.features.conv0B.bias.requires_grad = False\n",
    "\n",
    "#print('bias size:', model.features.conv0B.bias.data.size())\n",
    "\n",
    "print(model.features.conv0B.weight[16,0,0,:,:])\n",
    "print('Update param: ',model.features.conv0W.weight[0,:,:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number-of-parameters: [1530]\n",
      "All parameter_name: features.conv0B.weight\n",
      "All parameter_name: features.conv0B.bias\n",
      "All parameter_name: features.conv0W.weight\n",
      "All parameter_name: features.conv0W.bias\n",
      "All parameter_name: features.bn0.weight\n",
      "All parameter_name: features.bn0.bias\n",
      "All parameter_name: features.bn0.running_mean\n",
      "All parameter_name: features.bn0.running_var\n",
      "All parameter_name: classifier.weight\n",
      "All parameter_name: classifier.bias\n",
      "Gradient updated parameter_name: features.conv0W.weight\n",
      "Gradient updated parameter_name: features.conv0W.bias\n",
      "Gradient updated parameter_name: features.bn0.weight\n",
      "Gradient updated parameter_name: features.bn0.bias\n",
      "Gradient updated parameter_name: classifier.weight\n",
      "Gradient updated parameter_name: classifier.bias\n"
     ]
    }
   ],
   "source": [
    "print('number-of-parameters:',log['num_params'])\n",
    "        \n",
    "for name1, param1 in model.state_dict().items():\n",
    "    print('All parameter_name:',name1)\n",
    "    \n",
    "\n",
    "for name, param in model.named_parameters(): #for name, param in model.state_dict().items():\n",
    "    if param.requires_grad:\n",
    "        print('Gradient updated parameter_name:',name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "----------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 5-dimensional weight for 5-dimensional input [100, 1, 3, 32, 32], but got weight of size [18, 1, 1, 1, 3, 3] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-5813662d5c3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-b0605734ea4e>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, optimizer, log_saver, num_epochs, margin_dist_ind)\u001b[0m\n\u001b[0;32m     33\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m                 \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m                 \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3.6\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-167453af7e97>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3.6\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3.6\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3.6\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3.6\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    419\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m         return F.conv3d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 421\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 5-dimensional weight for 5-dimensional input [100, 1, 3, 32, 32], but got weight of size [18, 1, 1, 1, 3, 3] instead"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# we should optim the params that need gradient because we set some param to be requires_grad False\n",
    "params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = optim.SGD(params, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "model, log = train_model(model, criterion, optimizer, log, num_epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
